{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":13,"outputs":[{"output_type":"stream","text":"/kaggle/input/allstate-purchase-prediction-challenge/train.csv.zip\n/kaggle/input/allstate-purchase-prediction-challenge/sampleSubmission.csv\n/kaggle/input/allstate-purchase-prediction-challenge/test_v2.csv.zip\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Allstate Purchase Prediction Challenge\n# Author: Alessandro Mariani <alzmcr@yahoo.it>\n# https://www.kaggle.com/c/allstate-purchase-prediction-challenge\n\n\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport operator\n\nfrom time import time\nfrom sklearn import ensemble\n\nimport multiprocessing, operator\nimport pandas as pd, numpy as np\n\n## Pickle FIX for multiprocessing using bound methods\n## http://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-pythons-multiprocessing-pool-ma/\nfrom copy_reg import pickle\nfrom types import MethodType\nfrom time import time\nfrom itertools import combinations\nfrom sklearn import preprocessing\n\nimport scipy as sp, numpy as np, pandas as pd\n\n# Cantor Pairing\ndef cantor(args):\n    # Cantor Pairing - recursive call if more than 1 pair\n    if len(args) > 2:\n        x2 = cantor(args[1:])\n        x1 = args[0]\n    else:\n        x1, x2 = args\n    return int((0.5 * (x1 + x2)*(x1 + x2 + 1) + x2))\n\n# Groups all columns of data into combinations of [degree]\ndef group_data(data, degree=3, hash=hash, NAMES=None): \n    init = time()\n    new_data = []; combined_names = []\n    m,n = data.shape\n    for indicies in combinations(range(n), degree):\n        new_data.append([hash(tuple(v)) for v in data[:,indicies]])\n        if NAMES != None:\n            combined_names.append( '+'.join([NAMES[indicies[i]] for i in range(degree)]) )\n    print =(\"DONE! %.2fm\",((time()-init)/60))\n    if NAMES != None:\n        return (np.array(new_data).T, combined_names)\n    return np.array(new_data).T\n\n# Return concatenated fields in a dataframe\n# [1,2,3,4,5,6] => '123456'\ndef concat(df, columns):\n    return np.array([''.join(x) for x in np.array(\n        [np.array(df[col].values, dtype=str) for col in columns]).T])\n\n# Breakfast Pirate Awesome State trick + some additions\ndef stateFix(encoders,df,c=['C','D','G'],verbose=False):\n    # GA\n    iGA = df.state == encoders['state'].transform(['GA'])[0]\n    ifix = iGA&(df[c[0]]==1); df.ix[ifix,c[0]] = 2; nga1 = np.sum(ifix) #C\n    ifix = iGA&(df[c[1]]==1); df.ix[ifix,c[1]] = 2; nga2 = np.sum(ifix) #D\n    # FL\n    iFL = df.state == encoders['state'].transform(['FL'])[0]\n    ifix = iFL&(df[c[2]]<=2); df.ix[ifix,c[2]] = 3; nfl1 = np.sum(ifix) #G\n    # OH\n    iOH = df.state == encoders['state'].transform(['OH'])[0]\n    ifix = iOH&(df[c[2]]==1); df.ix[ifix,c[2]] = 2; noh1 = np.sum(ifix) #G\n    # ND\n    iND = df.state == encoders['state'].transform(['ND'])[0]\n    ifix = iND&(df[c[2]]!=2); df.ix[ifix,c[2]] = 2; nnd1 = np.sum(ifix) #G\n    # SD\n    iSD = df.state == encoders['state'].transform(['SD'])[0]\n    ifix = iSD&(df[c[2]]!=2); df.ix[ifix,c[2]] = 2; nsd1 = np.sum(ifix) #G\n    if verbose:\n        print =(\"Fixed state law products. GA1:%i GA2:%i FL1:%i OH1:%i ND1:%i SD1:%i\",(\n            nga1, nga2, nfl1, noh1, nnd1, nsd1))\n\n# Target variable expected value given a categorical feature\ndef expval(df,col,y,tfilter):\n    tmp = pd.DataFrame(index=df.index)\n    pb = df[tfilter][y].mean()                                              # train set mean\n    tmp['cnt'] = df[col].map(df[tfilter][col].value_counts()).fillna(0)     # train set count\n    tmp['csm'] = df[col].map(df[tfilter].groupby(col)[y].sum()).fillna(pb)  # train set sum\n    tmp.ix[tfilter,'cnt'] -= 1                                              # reduce count for train set\n    tmp.ix[tfilter,'csm'] -= df.ix[tfilter,y]                               # remove current value\n    tmp['exp'] = ((tmp.csm+ pb*15) / (tmp.cnt+ 15)).fillna(pb)              # calculate mean including kn-extra 'average' samples \n    np.random.seed(1)\n    tmp.ix[tfilter,'exp'] *= 1+.3*(np.random.rand(len(tmp[tfilter]))-.5) # add some random noise to the train set\n    return tmp.exp\n\ndef prepare_data(shuffle=True):\n    alltest = pd.read_csv('data\\\\test_v2.csv')\n    test = alltest.set_index('customer_ID')\n    alldata = pd.read_csv('data\\\\train.csv').set_index('customer_ID')\n\n    # handy lists of features\n    con = ['group_size','car_age','age_oldest','age_youngest','duration_previous','cost']\n    cat = ['homeowner','car_value','risk_factor','married_couple','C_previous','state', 'location','shopping_pt']\n    conf = ['A','B','C','D','E','F','G']; conf_f = [col+'_f' for col in conf]\n    extra = []\n\n    final_purchase = alldata[alldata.record_type == 1]          # final purchase\n    data = alldata.join(final_purchase[conf], rsuffix='_f')     # creating training dataset with target features\n    data = data[data.record_type == 0]                          # removing final purchase\n\n    data['conf'] = concat(data,conf_f)                          # handy purchase plan \n    data['conf_init'] = concat(data,conf)                       # handy last quoted plan\n\n    encoders = dict()\n    data = data.append(test)\n\n    # Fix NAs\n    data['C_previous'].fillna(0, inplace=1)\n    data['duration_previous'].fillna(0, inplace=1)\n    data.location.fillna(-1, inplace=1);\n    # Transform data to numerical data\n    for col in ['car_value','risk_factor','state']:\n        encoders[col] = preprocessing.LabelEncoder()\n        data[col] = encoders[col].fit_transform(data[col].fillna(99))\n\n    print =('Location substitution:'),\n    ## get rid of very location, given the total count from train,cv and test set\n    x = data[data.shopping_pt==2].location.value_counts()\n    sub = data.location.map(x).fillna(0) < 5\n    data.ix[sub,'location'] = data.state[sub]; print =('%.5f',sub.mean())\n\n    # cost per car_age; cost per person; cost per state\n    data['caCost'] = 1.*data.cost / (data.car_age+1)\n    data['ppCost'] = 1.*data.cost / data.group_size\n    data['stCost'] = data.state.map(data.groupby('state')['cost'].mean())\n    extra.extend(['caCost','ppCost','stCost'])\n\n    # average quote cost by G values\n    data['costG'] = data['G'].map(data.groupby('G')['cost'].mean())\n    extra.append('costG')\n\n    # average quote cost by G & state values\n    x = data.groupby(['G','state'])['cost'].mean()\n    x = x.reset_index().set_index(['G','state']); x.columns = ['costStG']   # covert to DF\n    data = data.merge(x,left_on=['G','state'],right_index=True,how='left')\n    extra.append('costStG')\n\n    # two way intersactino between state, G and shopping_pt\n    print =(\"Grouping few 2-way interactions...\"),\n    grpTrn, c2 = group_data(data[['state','G','shopping_pt']].values,2,hash,['state','G','shopping_pt'])\n    for i,col in enumerate(c2):\n        encoders[col] = preprocessing.LabelEncoder()\n        data[col] = encoders[col].fit_transform(grpTrn[:,i])\n    extra.extend(c2)\n\n    # expected value (arithmetic average) of G by state & location\n    for col in ['state','location']:\n        extra.append(col+'_exp')\n        data[col+'_exp'] = expval(data,col,'G_f',-data.G_f.isnull())\n\n    # previous G\n    data['prev_G'] = data.G.shift(1); extra.append('prev_G')\n    data.ix[data.shopping_pt == 1,'prev_G'] = data.ix[data.shopping_pt==1,'G']\n\n    # separating training & test data\n    test = data[data.conf.isnull()]; data = data[-data.conf.isnull()]\n\n    # SHUFFLE THE DATASET, keeping the same customers transaction in order\n    if shuffle:\n        print =(\"Shuffling dataset...\"),\n        np.random.seed(9); ids = np.unique(data.index.values)\n        rands = pd.Series(np.random.random_sample(len(ids)),index=ids)\n        data['rand'] = data.reset_index()['customer_ID'].map(rands).values\n        data.sort(['rand','shopping_pt'],inplace=1); print =(\"DONE!\")\n\n    # convert to int due to emtpy values in test set\n    for col in conf_f: data[col] = np.array(data[col].values,dtype=np.int8)\n\n    return data,test,con,cat,extra,conf,conf_f,encoders\n\n        \ndef _pickle_method(method):\n    func_name = method.im_func.__name__\n    obj = method.im_self\n    cls = method.im_class\n    return _unpickle_method, (func_name, obj, cls)\n\ndef _unpickle_method(func_name, obj, cls):\n    for cls in cls.mro():\n        try:\n            func = cls.__dict__[func_name]\n        except KeyError:\n            pass\n        else:\n            break\n    return func.__get__(obj, cls)\n\nclass RandomForestsParallel(object):\n    # class used to fit & predict in parallel minimizing memory usage\n    rfs = []\n    def __init__(self,N,ntree,maxfea,leafsize,N_proc=None):\n        self.N = N\n        self.ntree = ntree; self.maxfea = maxfea; self.leafsize = leafsize\n        self.N_proc = N_proc if N_proc is not None else max(1,multiprocessing.cpu_count()-1)\n\n        # fix pickling when using bound methods in classes\n        pickle(MethodType, _pickle_method, _pickle_method)\n\n    def _parallel_fit(self, rf):\n        t = time()\n        return rf.fit(self.X,self.y,self.w), (time()-t)/60.\n\n    def _parallel_predict(self, rf):\n        return rf.predict(self.X)\n    \n    def fit(self,X,y,w=None):\n        # fit N random forest in parallel\n        self.rfs = []; self.X = X; self.y = y\n        self.w = np.ones(y.shape,dtype=bool) if w is None else w\n        print =(\"fitting %i RFs using %i processes...\",(self.N,self.N_proc)),\n\n        args = [ensemble.RandomForestClassifier(\n            n_estimators=self.ntree, max_features=self.maxfea,\n            min_samples_leaf=self.leafsize,random_state=irf,\n            compute_importances=1) for irf in range(self.N)]\n\n        if self.N_proc > 1:\n            pool = multiprocessing.Pool(self.N_proc)\n            for i,(rf,irft) in enumerate(pool.imap(self._parallel_fit,args)):\n                self.rfs.append(rf); print =(\"rf#%i %.2fm\",(i,irft)),\n            pool.terminate()\n        else:\n            for i,rf in enumerate(args):\n                rf,irft = self._parallel_fit(rf)\n                self.rfs.append(rf); print=(\"rf#%i %.2fm\",(i,irft)),\n                \n        del self.X,self.y,self.w\n        # set the importances of the features\n        self.impf = self._calculate_impf(X.columns)\n\n        return self\n        \n    def predict(self,X,single_process=True):\n        # predict using all the random forest in self.rfs\n        # single_process is set by default, as multiprocess predict is not\n        # memory efficient and sometime time efficient (efficient smaller N)\n        self.X = X\n        if (not single_process) & (self.N_proc > 1):\n            pool = multiprocessing.Pool(self.N_proc)\n            allpreds = np.array([p for p in pool.imap(self._parallel_predict,self.rfs)]).T\n            pool.terminate()\n        else:\n            allpreds = np.array([self._parallel_predict(rf) for rf in self.rfs]).T\n            \n        del self.X\n        \n        return allpreds\n\n    def _calculate_impf(self, feature_names):\n        # private method to calculate the average features importance\n        return pd.Series(reduce(operator.add,[rf.feature_importances_ for rf in self.rfs]) / self.N, feature_names)\n\n    def __repr__(self):\n        return (\"N:%i N_proc:%i ntree:%i maxfea:%i leafsize:%i fitted:%s\",(    \n            self.N, self.N_proc, self.ntree,self.maxfea,\n            self.leafsize, 'Yes' if len(self.rfs) > 0 else 'No'))\n\nfrom sklearn.model_selection import cross_validate,\nfrom utils import prepare_data,\nfrom parallel import RandomForestsParallel\nfrom time import time\n\ndef majority_vote(baseline,model_predictions):\n    # given a baseline and a matrix of prediction (#samples x #models)\n    # if will return the prediction if 1+#models/2 agree on the same product\n    # otherwise will return the baseline\n    prcnt = np.vstack([np.bincount(p,minlength=5) for p in model_predictions])\n    prmax = np.max(prcnt,axis=1) >= (1+(len(selected)/2))\n    preds = baseline+0; preds[prmax] = np.argmax(prcnt[prmax],axis=1)\n    return preds\n\ndef make_ptscores(y_true,y_pred,y_base,pt,vmask):\n    # measure the increase of \"plan\" accuracy given a prediction for the product (G)\n    return [np.mean(vmask[pt==ipt]&(y_true[pt==ipt] == y_pred[pt==ipt])) - np.mean(vmask[pt==ipt]&(y_true[pt==ipt] == y_base[pt==ipt])) for ipt in range(1,11)]\n\nif __name__ == '__main__':\n    ############################################################################\n    ## SETTING #################################################################\n    # submit: if 'True' create a submission file and train models for submission\n    # N: number of models to build\n    # NS: number of models to selected for majority vote\n    # kfold: number of k-fold to perform, if not submitting\n    # N_proc: number of process to spawn, default #CPU(s)-1\n    # include_from_pt: minimum shopping_pt included in the data set\n    # verbose_selection: print all details while selecting the model\n    # tn: test set distrubution of shopping_pt (#10-11 merged)    \n    ############################################################################\n    submit = True; N = 50; NS = 9; kfold = 3; N_proc = None;\n    include_from_pt = 1; verbose_selection = False\n    tn = np.array([18943,13298,9251,6528,4203,2175,959,281,78])\n    ############################################################################\n    # Random Forest Setting ####################################################\n    # Must be a list containg a tuple with (ntree,maxfea,leafsize)\n    params = [(50,5,23)]\n    # ex. [(x,5,23) for x in [35,50,75]] # [(50,x,23) for x in range(4,12)]\n    # anything you'd like to try, here is the place for the modifications\n    ############################################################################\n    \n    print=(\"Majority vote using %i models, selecting %i\\n\",(N,NS))\n    # initialize data\n    data,test,con,cat,extra,conf,conf_f,encoders = prepare_data()\n    data = data[data.shopping_pt >=include_from_pt]; print=(\"Including from shopping_pt #%i\\n\",data.shopping_pt.min()),\n    # features, target, weights (not used)\n    X = data[con+cat+conf+extra]; y = data['G_f'] ; w = np.ones(y.shape)\n    \n    vmask = reduce(operator.and_,data[conf[:-1]].values.T==data[conf_f[:-1]].values.T)\n    scores,imp,ptscores = {},{},{}\n    for n,m,l in params:\n        t = time();\n        scores[(m,l)],imp[(m,l)],ptscores[(m,l)] = [],[],[]\n        col_trscores,col_cvscores = [],[]\n\n        # initialize the ensemble of forests to run in parallel\n        # class is also structured to handle single-process \n        rfs = RandomForestsParallel(N, n, m, l, N_proc)\n        \n        # cross validation is use to find the best parameters\n        for ifold,(itr,icv) in enumerate(cross_validation.KFold(len(y),kfold,indices=False)):\n            if submit:\n                # just a lame way to re-using the same code for fitting & selecting when submitting :)\n                itr = np.ones(y.shape,dtype=bool); icv = -itr\n                print=(\"\\nHEY! CREATING SUBMISSION!\\n\")\n            else:\n                # redo expected value for the current training & cv set\n                for c in [x for x in X.columns if x[-4:] == '_exp']:\n                    X[c] = expval(data,c[:-4],'G_f',itr)           \n\n            # fits the random forests at the same time\n            rfs.fit(X[itr],y[itr],w[itr])\n\n            print=(\"predicting...\"),\n            allpreds = rfs.predict(X)\n            rftscores = []\n            print=(\"selecting models...\")\n            for irf in range(len(rfs.rfs)):\n                # SELECTION of the best random forest, even though probably\n                # is just getting rid of very unlucky seeds ...\n                pG = allpreds[:,irf]; ipt2 =  data.shopping_pt > 1\n                ptscore = make_ptscores(y[icv],pG[icv],data.G[icv],data.shopping_pt[icv],vmask[icv])\n                tptscore = make_ptscores(y[itr],pG[itr],data.G[itr],data.shopping_pt[itr],vmask[itr])\n                rftscores.append((tn.dot(tptscore[1:]),irf))\n                print =(\"%i,%i %.5f %.5f %.5f %.5f\"),(\n                    ifold,irf,\n                    np.mean(pG[itr]==y[itr]),np.mean(vmask[itr]&(pG[itr]==y[itr])),\n                    np.mean(pG[ipt2&itr]==y[ipt2&itr]),np.mean(vmask[ipt2&itr]&(pG[ipt2&itr]==y[ipt2&itr]))),\n                if verbose_selection:\n                    print =(\" \".join([\"%.5f\" %pts for pts in ptscore])),\n                    print =(\" \".join([\"%.5f\" %pts for pts in tptscore])),\n                print =(\"%.2f %.2f\",(tn.dot(tptscore[1:]),tn.dot(ptscore[1:])))\n\n            # select the best models for the majority vote\n            rftscores.sort(reverse=1); selected = [x[1] for x in rftscores[:NS]]\n\n            print =(\"counting votes...\")\n            # print also the score using all the models\n            pG = majority_vote(data.G,allpreds)\n            ptscore = make_ptscores(y[icv],pG[icv],data.G[icv],data.shopping_pt[icv],vmask[icv])\n            # ifold,a : majority vote score using all models\n            print =(str(ifold)+\",a \"+\" \".join([\"%.5f\" %pts for pts in ptscore])+\" %.2f\" % tn.dot(ptscore[1:]))\n            \n            # results for selected models\n            pG = majority_vote(data.G,allpreds[:,selected])\n            ptscore = make_ptscores(y[icv],pG[icv],data.G[icv],data.shopping_pt[icv],vmask[icv])\n            # ifold,s : majority vote score using selected models\n            print =(str(ifold)+\",s \"+\" \".join([\"%.5f\" %pts for pts in ptscore])+\" %.2f\" % tn.dot(ptscore[1:]))\n            \n            # append features importances & scores\n            col_trscores.append(np.mean(pG[itr]==y[itr]))        # append train score\n            col_cvscores.append(np.mean(pG[icv]==y[icv]))        # append cv score\n            imp[(m,l)].append(rfs.impf)\n            scores[(m,l)].append(tn.dot(ptscore[1:]))\n            ptscores[(m,l)].append(ptscore)\n\n            # skip any following fold if we're submitting\n            if submit: break\n        \n        print=(\"%i %i %i\\t %.2f %.2f %.4f %.4f %.2f - %.2fm\",(\n            n,m,l,\n            np.mean(scores[(m,l)]), np.std(scores[(m,l)]),  # for best params & variance\n            np.mean(col_trscores), np.mean(col_cvscores),   # use x diagnostic training set overfit\n            tn.dot(np.mean(ptscores[(m,l)],axis=0)[1:])),    # score\n            (time()-t)/60),                                 # k-fold time\n        print =(\" \".join([\"%.5f\" %pts for pts in np.mean(ptscores[(m,l)],axis=0)])),\n        print =(\" \".join([\"%.5f\" %pts for pts in np.std(ptscores[(m,l)],axis=0)]))\n        \n    if submit:\n        # MAKE SUBMISSION\n        # very complicated way to keep only the latest shopping_pt for each customer just to have everything in one row!!!!!11\n        test = test[test.shopping_pt == test.reset_index().customer_ID.map(test.reset_index().groupby('customer_ID').shopping_pt.max())]\n        Xt = test[con+cat+conf+extra]\n\n        # TEST SET PREDICTION\n        print =(\"now predicting on test set...\"),\n        allpreds = rfs.predict(Xt)\n        test['pG'] = majority_vote(test.G,allpreds[:,selected]); print =(\"done\")\n        \n        # Fix state law products, then concatenate to string\n        stateFix(encoders,test,['C','D','pG'],1)\n        test['plan'] = concat(test,['A','B','C','D','E','F','pG'])\n        test['plan'].to_csv('submission\\\\majority_rfs%i_%i.%i_shuffle_GAfix_%iof%iof%i.csv' % (\n            n,m,l,NS/2+1,NS,N),header=1)\n\n        # features importances\n        impf = rfs.impf; impf.sort()\n        test.to_csv('submission.csv')","execution_count":14,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"trailing comma not allowed without surrounding parentheses (<ipython-input-14-1582cf12b45d>, line 263)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-1582cf12b45d>\"\u001b[0;36m, line \u001b[0;32m263\u001b[0m\n\u001b[0;31m    from sklearn.model_selection import cross_validate,\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}